{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.config.Config at 0x286dacb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global configurations for sklearn:\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "# Global configurations for pandas:\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Global configurations for polars:\n",
    "pl.Config.activate_decimals(True).set_tbl_hide_column_data_types(True)\n",
    "pl.Config(\n",
    "    **dict(tbl_formatting = 'ASCII_FULL_CONDENSED',\n",
    "            tbl_hide_column_data_types = False,\n",
    "            tbl_hide_dataframe_shape = True,\n",
    "            fmt_float = \"mixed\",\n",
    "            tbl_cell_alignment = 'CENTER',\n",
    "            tbl_hide_dtype_separator = True,\n",
    "            tbl_cols = 100,\n",
    "            tbl_rows = 50,\n",
    "            fmt_str_lengths = 100,\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data/\"\n",
    "TRAIN_DIR = ROOT_DIR + \"train/\"\n",
    "TEST_DIR = ROOT_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameProcessor:\n",
    "    \"\"\" Dataframe processing class.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_types(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Converts columns' data types for memory.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column == \"target\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int8))\n",
    "            elif column in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int32))\n",
    "            elif column == \"date_decision\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))\n",
    "            elif column[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Float64))\n",
    "            elif column[-1] == \"M\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.String))\n",
    "            elif column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_processor(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Processes the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "\n",
    "            if column == \"date_decision\":\n",
    "                df = df.with_columns(\n",
    "                    pl.col(column).dt.year().alias(\"year_decision\"),\n",
    "                    pl.col(column).dt.month().alias(\"month_decision\"),\n",
    "                    pl.col(column).dt.day().alias(\"day_decision\"),\n",
    "                    pl.col(column).dt.week().alias(\"week_decision\"),\n",
    "                    pl.col(column).dt.weekday().alias(\"weekday_decision\"),\n",
    "                    pl.col(column).dt.quarter().alias(\"quarter_decision\")\n",
    "                )\n",
    "\n",
    "        df = df.drop([\"date_decision\", \"WEEK_NUM\", \"MONTH\"])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def delete_nulls_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Deletes columns with more than 80% of null values.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column not in [\"case_id\", \"target\"]:\n",
    "                null_percentage = df[column].is_null().sum() / df.shape[0]\n",
    "\n",
    "                if null_percentage > 0.80:\n",
    "                    df = df.drop(column)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_duplicates(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Drops duplicates from the DataFrame.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        df = df.unique(keep=\"first\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_columns_with_too_many_categories(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Drops columns with more than 100 categories or just 1.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if (column not in [\"target\", \"case_id\"]) & (df[column].dtype == pl.String):\n",
    "                categories_count = df[column].n_unique()\n",
    "\n",
    "                if (categories_count == 1) | (categories_count > 200):\n",
    "                    df = df.drop(column)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \"\"\"Dataframe aggreagating class.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def max_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the maximum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def min_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the minimum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_min = [pl.min(column).alias(f\"min_{column}\") for column in columns]\n",
    "\n",
    "        return expr_min\n",
    "\n",
    "    @staticmethod\n",
    "    def date_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"D\"]\n",
    "\n",
    "        expr_date = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_date\n",
    "\n",
    "    @staticmethod\n",
    "    def string_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the string columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"M\"]\n",
    "\n",
    "        expr_string = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_string\n",
    "\n",
    "    @staticmethod\n",
    "    def others_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the other columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column in (\"T\", \"L\")]\n",
    "\n",
    "        expr_others = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_others\n",
    "\n",
    "    @staticmethod\n",
    "    def count_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the count of rows.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if \"num_group\" in column]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def agg_expr(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by all the columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        expr_all = (\n",
    "            Aggregator.max_agg(df)\n",
    "            + Aggregator.min_agg(df)\n",
    "            + Aggregator.date_agg(df)\n",
    "            + Aggregator.string_agg(df)\n",
    "            + Aggregator.others_agg(df)\n",
    "            + Aggregator.count_agg(df)\n",
    "        )\n",
    "        return expr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str, depth: int = 0) -> pl.DataFrame:\n",
    "    \"\"\" Read file from the given path.\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): The path to the file.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(DataFrameProcessor.convert_types)\n",
    "\n",
    "    if depth in (1, 2):\n",
    "        df = df.group_by('case_id').agg(Aggregator.agg_expr(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_files(path: str, depth: int = 0) -> pl.DataFrame:\n",
    "    \"\"\" Read multiple files from the given path and concatenate them vertically.\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): The path to the files.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for one_path in glob(path):\n",
    "        df = read_file(one_path, depth)\n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.pipe(DataFrameProcessor.drop_duplicates)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(df_base: pl.DataFrame, *depth):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    \n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 43.1 ms, total: 149 ms\n",
      "Wall time: 151 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  target\n",
       "0        0    2019-01-03  201901         0       0\n",
       "1        1    2019-01-03  201901         0       0\n",
       "2        2    2019-01-04  201901         0       0\n",
       "3        3    2019-01-03  201901         0       0\n",
       "4        4    2019-01-04  201901         0       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test pandas reading time\n",
    "test_pandas_df = pd.read_parquet('data/train/train_base.parquet')\n",
    "test_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>date_decision</th><th>MONTH</th><th>WEEK_NUM</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>1</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>2</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>3</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>4</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "+---------+---------------+--------+----------+--------+\n",
       "| case_id | date_decision |  MONTH | WEEK_NUM | target |\n",
       "|   i64   |      str      |   i64  |    i64   |   i64  |\n",
       "+======================================================+\n",
       "|    0    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    1    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    2    |   2019-01-04  | 201901 |     0    |    0   |\n",
       "|    3    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    4    |   2019-01-04  | 201901 |     0    |    1   |\n",
       "+---------+---------------+--------+----------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.3 ms, sys: 31 ms, total: 100 ms\n",
      "Wall time: 57.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_polars_df = pl.read_parquet('data/train/train_base.parquet')\n",
    "display(test_polars_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment: Polars read data faster than pandas, since there are many files, it might be a good idea for me to mitigate to Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 1min, total: 3min 31s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dict = {\n",
    "    \"train_base\": read_file(TRAIN_DIR + \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR + \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR + \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR + \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR + \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR + \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR + \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.6 ms, sys: 109 ms, total: 171 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_dict = {\n",
    "    \"df_base\": read_file(TEST_DIR + \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR + \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR + \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR + \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR + \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR + \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR + \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* [Home Credit Baseline](https://www.kaggle.com/code/greysky/home-credit-baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

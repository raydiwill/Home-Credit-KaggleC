{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from typing import List, Tuple\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.config.Config at 0x286b99410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global configurations for sklearn:\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Global configurations for pandas:\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Global configurations for polars:\n",
    "pl.Config.activate_decimals(True).set_tbl_hide_column_data_types(True)\n",
    "pl.Config(\n",
    "    **dict(\n",
    "        tbl_formatting=\"ASCII_FULL_CONDENSED\",\n",
    "        tbl_hide_column_data_types=False,\n",
    "        tbl_hide_dataframe_shape=True,\n",
    "        fmt_float=\"mixed\",\n",
    "        tbl_cell_alignment=\"CENTER\",\n",
    "        tbl_hide_dtype_separator=True,\n",
    "        tbl_cols=100,\n",
    "        tbl_rows=50,\n",
    "        fmt_str_lengths=100,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data/\"\n",
    "TRAIN_DIR = ROOT_DIR + \"train/\"\n",
    "TEST_DIR = ROOT_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameProcessor:\n",
    "    \"\"\"Dataframe processing class.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_types(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Converts columns' data types for memory.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column == \"target\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int8))\n",
    "            elif column in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int32))\n",
    "            elif column == \"date_decision\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))\n",
    "            elif column[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Float64))\n",
    "            elif column[-1] == \"M\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.String))\n",
    "            elif column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def date_processor(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Processes the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(column) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(column).dt.total_days())\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Float32))\n",
    "\n",
    "            if column == \"date_decision\":\n",
    "                df = df.with_columns(\n",
    "                    pl.col(column).dt.year().alias(\"year_decision\"),\n",
    "                    pl.col(column).dt.month().alias(\"month_decision\"),\n",
    "                    pl.col(column).dt.day().alias(\"day_decision\"),\n",
    "                    pl.col(column).dt.week().alias(\"week_decision\"),\n",
    "                    pl.col(column).dt.weekday().alias(\"weekday_decision\"),\n",
    "                    pl.col(column).dt.quarter().alias(\"quarter_decision\"),\n",
    "                )\n",
    "\n",
    "        df = df.drop([\"date_decision\", \"WEEK_NUM\", \"MONTH\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_nulls_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Deletes columns with more than 80% of null values.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column not in [\"case_id\", \"target\"]:\n",
    "                null_percentage = df[column].is_null().sum() / df.shape[0]\n",
    "\n",
    "                if null_percentage > 0.80:\n",
    "                    df = df.drop(column)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_duplicates(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Drops duplicates from the DataFrame.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        df = df.unique(keep=\"first\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_columns_with_too_many_categories(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Drops columns with more than 100 categories or just 1.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if (column not in [\"target\", \"case_id\"]) & (df[column].dtype == pl.String):\n",
    "                categories_count = df[column].n_unique()\n",
    "\n",
    "                if (categories_count == 1) | (categories_count > 200):\n",
    "                    df = df.drop(column)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \"\"\"Dataframe aggreagating class.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def max_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the maximum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def min_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the minimum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_min = [pl.min(column).alias(f\"min_{column}\") for column in columns]\n",
    "\n",
    "        return expr_min\n",
    "\n",
    "    @staticmethod\n",
    "    def date_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"D\"]\n",
    "\n",
    "        expr_date = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_date\n",
    "\n",
    "    @staticmethod\n",
    "    def string_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the string columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"M\"]\n",
    "\n",
    "        expr_string = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_string\n",
    "\n",
    "    @staticmethod\n",
    "    def others_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the other columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column in (\"T\", \"L\")]\n",
    "\n",
    "        expr_others = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_others\n",
    "\n",
    "    @staticmethod\n",
    "    def count_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by the count of rows.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if \"num_group\" in column]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def agg_expr(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\"Aggregates the DataFrame by all the columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        expr_all = (\n",
    "            Aggregator.max_agg(df)\n",
    "            + Aggregator.min_agg(df)\n",
    "            + Aggregator.date_agg(df)\n",
    "            + Aggregator.string_agg(df)\n",
    "            + Aggregator.others_agg(df)\n",
    "            + Aggregator.count_agg(df)\n",
    "        )\n",
    "        return expr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str, depth: int = 0) -> pl.DataFrame:\n",
    "    \"\"\"Read file from the given path.\n",
    "\n",
    "    Arguments:\n",
    "    path (str): The path to the file.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(DataFrameProcessor.convert_types)\n",
    "\n",
    "    if depth in (1, 2):\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.agg_expr(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_files(path: str, depth: int = 0) -> pl.DataFrame:\n",
    "    \"\"\"Read multiple files from the given path and concatenate them vertically.\n",
    "\n",
    "    Arguments:\n",
    "    path (str): The path to the files.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for one_path in glob(path):\n",
    "        df = read_file(one_path, depth)\n",
    "        chunks.append(df)\n",
    "\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.pipe(DataFrameProcessor.drop_duplicates)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframe(df_base: pl.DataFrame, **depths: dict) -> pl.DataFrame:\n",
    "    \"\"\"Join multiple dataframes together.\n",
    "\n",
    "    Arguments:\n",
    "    df_base (pl.DataFrame): The base DataFrame.\n",
    "    **depths (dict of lists of pl.DataFrame): Named groups of DataFrames to join with df_base.\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for _, depth_group in depths.items():\n",
    "        for df in depth_group:\n",
    "            if df is not None:\n",
    "                df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{index}\")\n",
    "                index += 1\n",
    "\n",
    "    # Here you would integrate any post-join operations if necessary\n",
    "    df_base = df_base.pipe(DataFrameProcessor.date_processor)\n",
    "\n",
    "    return df_base\n",
    "\n",
    "\n",
    "def convert_to_pandas_df(\n",
    "    polars_df: pl.DataFrame, category_columns: List[str] = None\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    pandas_df = polars_df.to_pandas()\n",
    "\n",
    "    # Apply the same set of column to the test set\n",
    "    if category_columns is None:\n",
    "        category_columns = list(pandas_df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "    # Convert to category for ML\n",
    "    pandas_df[category_columns] = pandas_df[category_columns].astype(\"category\")\n",
    "\n",
    "    return pandas_df, category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pandas_df(\n",
    "    polars_df: pl.DataFrame, category_columns: List[str] = None\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    pandas_df = polars_df.to_pandas()\n",
    "\n",
    "    # Apply the same set of column to the test set\n",
    "    if category_columns is None:\n",
    "        category_columns = list(pandas_df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "    # Convert to category for ML\n",
    "    pandas_df[category_columns] = pandas_df[category_columns].astype(\"category\")\n",
    "\n",
    "    return pandas_df, category_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.4 ms, sys: 30.9 ms, total: 106 ms\n",
      "Wall time: 95.1 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  target\n",
       "0        0    2019-01-03  201901         0       0\n",
       "1        1    2019-01-03  201901         0       0\n",
       "2        2    2019-01-04  201901         0       0\n",
       "3        3    2019-01-03  201901         0       0\n",
       "4        4    2019-01-04  201901         0       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test pandas reading time\n",
    "test_pandas_df = pd.read_parquet('data/train/train_base.parquet')\n",
    "test_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>date_decision</th><th>MONTH</th><th>WEEK_NUM</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>1</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>2</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>3</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>4</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "+---------+---------------+--------+----------+--------+\n",
       "| case_id | date_decision |  MONTH | WEEK_NUM | target |\n",
       "|   i64   |      str      |   i64  |    i64   |   i64  |\n",
       "+======================================================+\n",
       "|    0    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    1    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    2    |   2019-01-04  | 201901 |     0    |    0   |\n",
       "|    3    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    4    |   2019-01-04  | 201901 |     0    |    1   |\n",
       "+---------+---------------+--------+----------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 ms, sys: 23.4 ms, total: 71.5 ms\n",
      "Wall time: 39.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_polars_df = pl.read_parquet('data/train/train_base.parquet')\n",
    "display(test_polars_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment: Polars read data faster than pandas, since there are many files, it might be a good idea for me to mitigate to Polars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>date_decision</th><th>MONTH</th><th>WEEK_NUM</th><th>target</th></tr><tr><td>i32</td><td>date</td><td>i64</td><td>i32</td><td>i8</td></tr></thead><tbody><tr><td>0</td><td>2019-01-03</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>1</td><td>2019-01-03</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>2</td><td>2019-01-04</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>3</td><td>2019-01-03</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>4</td><td>2019-01-04</td><td>201901</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "+---------+---------------+--------+----------+--------+\n",
       "| case_id | date_decision |  MONTH | WEEK_NUM | target |\n",
       "|   i32   |      date     |   i64  |    i32   |   i8   |\n",
       "+======================================================+\n",
       "|    0    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    1    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    2    |   2019-01-04  | 201901 |     0    |    0   |\n",
       "|    3    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    4    |   2019-01-04  | 201901 |     0    |    1   |\n",
       "+---------+---------------+--------+----------+--------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = read_file(TRAIN_DIR + \"train_base.parquet\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 35.3 s, total: 2min 17s\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dict = {\n",
    "    \"df_base\": read_file(TRAIN_DIR + \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR + \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR + \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR + \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR + \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR + \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR + \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR + \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 2.9 s, total: 10.9 s\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_master = merge_dataframe(**train_dict)\n",
    "train_df = train_master.pipe(DataFrameProcessor.delete_nulls_column).pipe(DataFrameProcessor.drop_columns_with_too_many_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 ms, sys: 29.2 ms, total: 79.2 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_dict = {\n",
    "    \"df_base\": read_file(TEST_DIR + \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR + \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR + \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR + \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR + \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR + \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR + \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR + \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 8.28 ms, total: 25.7 ms\n",
      "Wall time: 29.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = merge_dataframe(**test_dict)\n",
    "test_df = test_df.select([column for column in train_df.columns if column != \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 1.92 s, total: 6.26 s\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, used_category_columns = convert_to_pandas_df(train_df)\n",
    "test_df, _ = convert_to_pandas_df(test_df, used_category_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Home Credit Baseline](https://www.kaggle.com/code/greysky/home-credit-baseline)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.config.Config at 0x148bc8250>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global configurations for sklearn:\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "# Global configurations for pandas:\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Global configurations for polars:\n",
    "pl.Config.activate_decimals(True).set_tbl_hide_column_data_types(True)\n",
    "pl.Config(\n",
    "    **dict(tbl_formatting = 'ASCII_FULL_CONDENSED',\n",
    "            tbl_hide_column_data_types = False,\n",
    "            tbl_hide_dataframe_shape = True,\n",
    "            fmt_float = \"mixed\",\n",
    "            tbl_cell_alignment = 'CENTER',\n",
    "            tbl_hide_dtype_separator = True,\n",
    "            tbl_cols = 100,\n",
    "            tbl_rows = 50,\n",
    "            fmt_str_lengths = 100,\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"data/\"\n",
    "TRAIN_DIR = ROOT_DIR + \"train/\"\n",
    "TEST_DIR = ROOT_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameProcessor:\n",
    "    \"\"\" Dataframe processing class.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_types(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Converts columns' data types for memory.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column == \"target\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int8))\n",
    "            elif column in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Int32))\n",
    "            elif column == \"date_decision\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))\n",
    "            elif column[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Float64))\n",
    "            elif column[-1] == \"M\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.String))\n",
    "            elif column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(column).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_processor(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Processes the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "\n",
    "            if column == \"date_decision\":\n",
    "                df = df.with_columns(\n",
    "                    pl.col(column).dt.year().alias(\"year_decision\"),\n",
    "                    pl.col(column).dt.month().alias(\"month_decision\"),\n",
    "                    pl.col(column).dt.day().alias(\"day_decision\"),\n",
    "                    pl.col(column).dt.week().alias(\"week_decision\"),\n",
    "                    pl.col(column).dt.weekday().alias(\"weekday_decision\"),\n",
    "                    pl.col(column).dt.quarter().alias(\"quarter_decision\")\n",
    "                )\n",
    "\n",
    "        df = df.drop([\"date_decision\", \"WEEK_NUM\", \"MONTH\"])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def delete_nulls_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Deletes columns with more than 80% of null values.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column not in [\"case_id\", \"target\"]:\n",
    "                null_percentage = df[column].is_null().sum() / df.shape[0]\n",
    "\n",
    "                if null_percentage > 0.80:\n",
    "                    df = df.drop(column)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_duplicates(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Drops duplicates from the DataFrame.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        df = df.unique(keep=\"first\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_columns_with_too_many_categories(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\" Drops columns with more than 100 categories or just 1.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be processed.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if (column not in [\"target\", \"case_id\"]) & (df[column].dtype == pl.String):\n",
    "                categories_count = df[column].n_unique()\n",
    "\n",
    "                if (categories_count == 1) | (categories_count > 200):\n",
    "                    df = df.drop(column)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \"\"\"Dataframe aggreagating class.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def max_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the maximum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def min_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the minimum value.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_min = [pl.min(column).alias(f\"min_{column}\") for column in columns]\n",
    "\n",
    "        return expr_min\n",
    "\n",
    "    @staticmethod\n",
    "    def date_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the date columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"D\"]\n",
    "\n",
    "        expr_date = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_date\n",
    "\n",
    "    @staticmethod\n",
    "    def string_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the string columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column[-1] == \"M\"]\n",
    "\n",
    "        expr_string = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_string\n",
    "\n",
    "    @staticmethod\n",
    "    def others_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the other columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if column in (\"T\", \"L\")]\n",
    "\n",
    "        expr_others = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_others\n",
    "\n",
    "    @staticmethod\n",
    "    def count_agg(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by the count of rows.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        columns = [column for column in df.columns if \"num_group\" in column]\n",
    "\n",
    "        expr_max = [pl.max(column).alias(f\"max_{column}\") for column in columns]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def agg_expr(df: pl.DataFrame) -> pl.Expr:\n",
    "        \"\"\" Aggregates the DataFrame by all the columns.\n",
    "\n",
    "        Argument:\n",
    "        df (polars dataframe): The DataFrame to be aggregated.\n",
    "        \"\"\"\n",
    "        expr_all = (\n",
    "            Aggregator.max_agg(df)\n",
    "            + Aggregator.min_agg(df)\n",
    "            + Aggregator.date_agg(df)\n",
    "            + Aggregator.string_agg(df)\n",
    "            + Aggregator.others_agg(df)\n",
    "            + Aggregator.count_agg(df)\n",
    "        )\n",
    "        return expr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str, depth: int) -> pl.DataFrame:\n",
    "    \"\"\" Read file from the given path.\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): The path to the file.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(DataFrameProcessor.convert_types)\n",
    "\n",
    "    if depth in (1, 2):\n",
    "        df = df.group_by('case_id').agg(Aggregator.agg_expr(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_files(path: str, depth: int) -> pl.DataFrame:\n",
    "    \"\"\" Read multiple files from the given path and concatenate them vertically.\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): The path to the files.\n",
    "    depth (int): The depth of the file.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for one_path in glob(path):\n",
    "        df = read_file(one_path, depth)\n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.pipe(DataFrameProcessor.drop_duplicates)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.3 ms, sys: 71.3 ms, total: 126 ms\n",
      "Wall time: 150 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  target\n",
       "0        0    2019-01-03  201901         0       0\n",
       "1        1    2019-01-03  201901         0       0\n",
       "2        2    2019-01-04  201901         0       0\n",
       "3        3    2019-01-03  201901         0       0\n",
       "4        4    2019-01-04  201901         0       1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test pandas reading time\n",
    "test_pandas_df = pd.read_parquet('data/train/train_base.parquet')\n",
    "test_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>date_decision</th><th>MONTH</th><th>WEEK_NUM</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>1</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>2</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>3</td><td>&quot;2019-01-03&quot;</td><td>201901</td><td>0</td><td>0</td></tr><tr><td>4</td><td>&quot;2019-01-04&quot;</td><td>201901</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "+---------+---------------+--------+----------+--------+\n",
       "| case_id | date_decision |  MONTH | WEEK_NUM | target |\n",
       "|   i64   |      str      |   i64  |    i64   |   i64  |\n",
       "+======================================================+\n",
       "|    0    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    1    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    2    |   2019-01-04  | 201901 |     0    |    0   |\n",
       "|    3    |   2019-01-03  | 201901 |     0    |    0   |\n",
       "|    4    |   2019-01-04  | 201901 |     0    |    1   |\n",
       "+---------+---------------+--------+----------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1526659, 5)\n",
      "CPU times: user 71.5 ms, sys: 40.5 ms, total: 112 ms\n",
      "Wall time: 86.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_polars_df = pl.read_parquet('data/train/train_base.parquet')\n",
    "display(test_polars_df.head())\n",
    "\n",
    "print(test_polars_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment: Polars read data faster than pandas, since there are many files, it might be a good idea for me to mitigate to Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* [Home Credit Baseline](https://www.kaggle.com/code/greysky/home-credit-baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
